{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y42uYi3LY0aK"
      },
      "source": [
        "# Create a Bagging Model in order to overcome memory space problems\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXrfj3DjY-j2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "088308cf-53f1-4f07-f829-80add753aa81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting open3d\n",
            "  Downloading open3d-0.17.0-cp39-cp39-manylinux_2_27_x86_64.whl (420.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.5/420.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.9/dist-packages (from open3d) (3.7.1)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.9/dist-packages (from open3d) (1.2.2)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting ipywidgets>=8.0.4\n",
            "  Downloading ipywidgets-8.0.5-py3-none-any.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.3/138.3 KB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from open3d) (4.65.0)\n",
            "Requirement already satisfied: werkzeug>=2.2.3 in /usr/local/lib/python3.9/dist-packages (from open3d) (2.2.3)\n",
            "Collecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Requirement already satisfied: numpy>1.15 in /usr/local/lib/python3.9/dist-packages (from open3d) (1.22.4)\n",
            "Collecting pyquaternion\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Collecting dash>=2.6.0\n",
            "  Downloading dash-2.9.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.9/dist-packages (from open3d) (6.0)\n",
            "Collecting pillow>=9.3.0\n",
            "  Downloading Pillow-9.4.0-cp39-cp39-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.9/dist-packages (from open3d) (1.4.4)\n",
            "Collecting nbformat==5.7.0\n",
            "  Downloading nbformat-5.7.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 KB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.9/dist-packages (from nbformat==5.7.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.9/dist-packages (from nbformat==5.7.0->open3d) (4.3.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.9/dist-packages (from nbformat==5.7.0->open3d) (5.3.0)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.9/dist-packages (from nbformat==5.7.0->open3d) (2.16.3)\n",
            "Collecting dash-core-components==2.0.0\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-html-components==2.0.0\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-table==5.0.0\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.9/dist-packages (from dash>=2.6.0->open3d) (2.2.3)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from dash>=2.6.0->open3d) (5.13.1)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=8.0.4->open3d) (7.9.0)\n",
            "Collecting widgetsnbextension~=4.0\n",
            "  Downloading widgetsnbextension-4.0.6-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyterlab-widgets~=3.0 in /usr/local/lib/python3.9/dist-packages (from ipywidgets>=8.0.4->open3d) (3.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (23.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (1.4.4)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (4.39.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib>=3->open3d) (5.12.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.0->open3d) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21->open3d) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21->open3d) (1.1.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=2.2.3->open3d) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (8.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (2.1.2)\n",
            "Requirement already satisfied: importlib-metadata>=3.6.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (6.1.0)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.9/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (3.1.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib>=3->open3d) (3.15.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (2.6.1)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.8.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (67.6.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.9/dist-packages (from ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (4.4.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema>=2.6->nbformat==5.7.0->open3d) (22.2.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.2.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.7->matplotlib>=3->open3d) (1.16.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.9/dist-packages (from jupyter-core->nbformat==5.7.0->open3d) (3.1.1)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.9/dist-packages (from jedi>=0.10->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.8.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.9/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.9/dist-packages (from pexpect->ipython>=6.1.0->ipywidgets>=8.0.4->open3d) (0.7.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, widgetsnbextension, pyquaternion, pillow, jedi, configargparse, nbformat, ipywidgets, dash, open3d\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.2\n",
            "    Uninstalling widgetsnbextension-3.6.2:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.2\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 8.4.0\n",
            "    Uninstalling Pillow-8.4.0:\n",
            "      Successfully uninstalled Pillow-8.4.0\n",
            "  Attempting uninstall: nbformat\n",
            "    Found existing installation: nbformat 5.8.0\n",
            "    Uninstalling nbformat-5.8.0:\n",
            "      Successfully uninstalled nbformat-5.8.0\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed addict-2.4.0 configargparse-1.5.3 dash-2.9.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 ipywidgets-8.0.5 jedi-0.18.2 nbformat-5.7.0 open3d-0.17.0 pillow-9.4.0 pyquaternion-0.9.9 widgetsnbextension-4.0.6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install open3d\n",
        "import os\n",
        "import open3d\n",
        "from random import sample\n",
        "import numpy as np\n",
        "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgzAyE0TQUWR",
        "outputId": "b6759357-367c-42f4-92bd-0d27ec4905ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svujEdQ9X5Cq",
        "outputId": "4052277f-043f-40d8-8739-c34e58c7e3e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kKhTK4NY5T"
      },
      "source": [
        "# MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os30XLZHY-wN"
      },
      "outputs": [],
      "source": [
        "def model_definition():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "\n",
        "  # add convolutional layers\n",
        "  model.add(Conv3D(16, kernel_size=(3, 3, 3), padding='same', input_shape=(32, 32, 32, 1), activation='relu'))\n",
        "  model.add(Dropout(0.2))#Dropout\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "\n",
        "  model.add(Conv3D(32, kernel_size=(3, 3, 3), padding='same', activation='relu'))\n",
        "  model.add(Dropout(0.2))#Dropout\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "  \n",
        "  model.add(Conv3D(64, kernel_size=(3, 3, 3), padding='same', input_shape=(32, 32, 32, 1), activation='relu'))\n",
        "  model.add(Dropout(0.2))#Dropout\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "\n",
        "\n",
        "  # add flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # add dense layers\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dropout(0.2))#Dropout\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BGq12gqudKhR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbRo3Gh2NZ3c"
      },
      "source": [
        "# DATA PREPARATION:\n",
        "Splitting the train set into 5 set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGVDHGlPTbRc"
      },
      "outputs": [],
      "source": [
        "#Extrapolate the voxels from the file \n",
        "def process_off_file(filepath):\n",
        "\n",
        "    voxel = open3d.io.read_voxel_grid(filepath)\n",
        "    return voxel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1qsMWQ5Y-5F"
      },
      "outputs": [],
      "source": [
        "def getKPartitionsFolderized(folderName, K, folderTest):\n",
        "    \n",
        "    # dooing for all the models\n",
        "    baseDIR = os.path.dirname(\"/content/drive/MyDrive/\")\n",
        "    rootModelsDirName = os.path.join(baseDIR, folderName)\n",
        "\n",
        "    models = [\"bathtub\", \"bed\", \"chair\", \"desk\", \"dresser\",\n",
        "              \"monitor\", \"night_stand\", \"sofa\", \"table\", \"toilet\"]\n",
        "    # models = [\"desk\"]\n",
        "\n",
        "    # PUSHING ALL models in allModels\n",
        "    allModels = {}\n",
        "\n",
        "    for modelFolder in models:\n",
        "        # print(f'current modelFolderName= {modelFolder} and isTestFolder={isTestModel}')\n",
        "        # PRELIMINARY STEPS for getting the input folder and creating respective output folder\n",
        "\n",
        "        # 1) Getting INPUT FILES\n",
        "        inputDIR = os.path.join(\n",
        "            rootModelsDirName, modelFolder, \"test\" if folderTest else \"train\")\n",
        "        #print(f'Working on {modelFolder} in folder {inputDIR}')\n",
        "        #print(os.path.isdir(inputDIR))\n",
        "\n",
        "        # Getting the list of all mesh in the directory 'modelFolder'\n",
        "        inputModels = []\n",
        "        INPUT_EXTENTION = \".ply\"\n",
        "        # Iterate directory\n",
        "        for path in os.listdir(inputDIR):\n",
        "            # check if current path is an expected file\n",
        "            if os.path.isfile(os.path.join(inputDIR, path)) and os.path.join(inputDIR, path).endswith(INPUT_EXTENTION):\n",
        "                # append only the file name\n",
        "                inputModels.append(os.path.splitext(path)[0])\n",
        "        # print(2, inputModels)\n",
        "        allModels[modelFolder] = inputModels\n",
        "\n",
        "    # DIVIDING allModels into K Partitions\n",
        "    k_sets_indexes = [i for i in range(0, K)]\n",
        "    randomsGlobal = [0 for _ in range(0, K)]\n",
        "\n",
        "    # array di K dictionaries inizializzati a empty\n",
        "    k_sets = [{} for _ in range(0, K)]\n",
        "    for key in allModels:\n",
        "        #print(key, len(allModels[key]))\n",
        "        for v in allModels[key]:\n",
        "            # of the bucket\n",
        "            index = sample(k_sets_indexes, 1)[0]\n",
        "            if not k_sets[index].keys().__contains__(key):\n",
        "                k_sets[index][key] = []\n",
        "            k_sets[index][key].append(v)\n",
        "            # print(index)\n",
        "            randomsGlobal[index] += 1\n",
        "\n",
        "    #print(f\"Randoms: {randomsGlobal}\")\n",
        "    return k_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68hkTyBDS3ml"
      },
      "source": [
        "# Prepare validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6nuwE-OquBL"
      },
      "outputs": [],
      "source": [
        "#From a set create the train set\n",
        "def prepare_data(partition, middlefolder):\n",
        "\n",
        "  train = []\n",
        "  labels = []\n",
        "\n",
        "  #Read all the data from a single partition of the dataset\n",
        "  #Create the filepath for each \n",
        "  train_data_raw = []  \n",
        "  i = -1\n",
        "  ordered_keys = sorted(partition.keys())\n",
        "  for key in ordered_keys:\n",
        "    i = i + 1\n",
        "    for value in partition[key]:\n",
        "      filepath = f'/content/drive/MyDrive/ModelNet10_Voxel/{key}/{middlefolder}/{value}.ply'\n",
        "      points = process_off_file(filepath)\n",
        "      train_data_raw.append(points)\n",
        "      labels.append(i)      \n",
        "\n",
        "  # Transform the Voxel Grid into Numpy Array containing a list of voxels \n",
        "  train_data_numpy = []\n",
        "  \n",
        "  #First step: Each array to voxels \n",
        "  for i in range(len(train_data_raw)):\n",
        "    train_data_numpy.append(np.asarray(train_data_raw[i].get_voxels()))\n",
        "\n",
        "  array_train_ready = []\n",
        "  array_train_normalized = []\n",
        "  \n",
        "  #Create the compact 32x32x32 vector\n",
        "  for i in range(len(train_data_numpy)):\n",
        "    array_temp = np.zeros((32, 32, 32,1))\n",
        "    for j in range(len(train_data_numpy[i])):\n",
        "        array_temp[train_data_numpy[i][j].grid_index[0],train_data_numpy[i][j].grid_index[1],train_data_numpy[i][j].grid_index[2]] = 1\n",
        "    #Perform normalization\n",
        "    #mean = np.mean(array_temp, axis = 0)\n",
        "    #std = np.std(array_temp, axis = 0)\n",
        "    #array_train_ready.append((array_temp - mean)/std)\n",
        "    array_train_ready.append(array_temp)\n",
        "\n",
        "  array_train_ready = np.asarray(array_train_ready) \n",
        "  train = np.array(array_train_ready)\n",
        "\n",
        "  #From list to array\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  return train,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIYbeKFsNlao"
      },
      "outputs": [],
      "source": [
        "#Create a unique validation set for every model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg96kzP3Y_Hm"
      },
      "source": [
        "# MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xXBxUwSZcdk",
        "outputId": "b9745083-1d24-4fe4-cecb-8c6a3fa06737"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['night_stand', 'bathtub', 'monitor', 'dresser', 'table', 'bed', 'chair', 'sofa', 'toilet', 'desk']\n"
          ]
        }
      ],
      "source": [
        "#Create now several models using the train/validation data\n",
        "\n",
        "#Print Labels \n",
        "class_folders = os.listdir('/content/drive/MyDrive/ModelNet10_Voxel')\n",
        "print(class_folders)\n",
        "\n",
        "#Prepare DATA\n",
        "folder = 'ModelNet10_Voxel'\n",
        "\n",
        "#SPLIT THE TRAIN SET\n",
        "#Sets will contain an array of 5 elements, containing 5 partition of the Train set\n",
        "#Each of these partition is divided thanks to a map, that contains:\n",
        "#Key: the name of the class, Value: an array of path\n",
        "\n",
        "#1 ONLY\n",
        "sets = getKPartitionsFolderized(folder, 1, False)\n",
        "\n",
        "#Split the test set, in order to create a Validation Set and a Test set\n",
        "test = getKPartitionsFolderized(folder,2,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kaYf4IZULYA"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "X_validation, y_validation = prepare_data(test[1], \"test\")\n",
        "y_validation = to_categorical(y_validation, 10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeAuq2BatZkl"
      },
      "outputs": [],
      "source": [
        "def cutout_3d(volume, cutout_size=(8,8,8)):\n",
        "    # Shape is 32x32x32\n",
        "    x = np.random.randint(0, 32 - cutout_size[0])\n",
        "    y = np.random.randint(0, 32 - cutout_size[1])\n",
        "    z = np.random.randint(0, 32 - cutout_size[2])\n",
        "    cutout_cube = (x, y, z, cutout_size[0], cutout_size[1], cutout_size[2])\n",
        "    volume[x:x+cutout_size[0], y:y+cutout_size[1], z:z+cutout_size[2]] = 0\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmlNdQPruk1K"
      },
      "outputs": [],
      "source": [
        "idt = np.random.permutation(len(X_validation))\n",
        "X_validation, y_validation = X_validation[idt], y_validation[idt]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(in_channels=1, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv3d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.conv3 = nn.Conv3d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=2, stride=2)\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64, 64)\n",
        "        self.relu4 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "        #self.softmax = nn.Softmax(dim=10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.pool3(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc2(x)\n",
        "        #x = self.softmax(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "ySEqwPJaAJqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#MODEL IN PYTORCH\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ModelDefinition(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModelDefinition, self).__init__()\n",
        "        self.conv1 = nn.Conv3d(1, 32, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.dropout1 = nn.Dropout(0.2)\n",
        "        self.pool1 = nn.MaxPool3d( kernel_size=(3, 3, 3))\n",
        "        self.conv2 = nn.Conv3d(32, 64, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.dropout2 = nn.Dropout(0.2)\n",
        "        self.pool2 = nn.MaxPool3d( kernel_size=(3, 3, 3))\n",
        "        self.conv3 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=1)\n",
        "        self.dropout3 = nn.Dropout(0.2)\n",
        "        self.pool3 = nn.MaxPool3d(kernel_size=(3, 3, 3))\n",
        "        #self.flatten = nn.Flatten(1,4)\n",
        "        self.fc1 = nn.Linear(128, 128)\n",
        "        self.dropout4 = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.pool1(x)\n",
        "      \n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = self.dropout3(x)\n",
        "        x = self.pool3(x)\n",
        "      \n",
        "        x = x.view(x.size(0)) #[64,1]\n",
        "        #x = self.flatten(x)\n",
        "   \n",
        "        x = F.relu(self.fc1(x))\n",
        "       \n",
        "        x = self.dropout4(x)\n",
        "        x = self.fc2(x)\n",
        "        \n",
        "        return x"
      ],
      "metadata": {
        "id": "E08fdAKcwd7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train = prepare_data(sets[0], \"train\") \n",
        "y_train = to_categorical(y_train, 10)"
      ],
      "metadata": {
        "id": "AuPoZGMGxHBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train.shape\n",
        "X_train = np.transpose(X_train, (0, 4, 1, 2,3))"
      ],
      "metadata": {
        "id": "I426BuQUuc1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_validation = np.transpose(X_validation, (0,4,1,2,3))"
      ],
      "metadata": {
        "id": "Du7JfLZ4wbkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Preparing the data for the numpy\n",
        "\n",
        "\n",
        "X_train_tensor = torch.from_numpy(X_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "#print(X_train_tensor.shape)\n",
        "num_samples = X_train_tensor.shape[0]\n",
        "\n",
        "X_validation_tensor = torch.from_numpy(X_validation).float()\n",
        "y_validation_tensor = torch.from_numpy(y_validation).float()\n",
        "num_validation = X_validation_tensor.shape[0]\n",
        "print(num_validation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "776pCqnW2YzO",
        "outputId": "c88ce5d1-ae01-4ac5-a5ea-2d52fc0c73fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "449\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "network = ModelDefinition()\n",
        "summary(network, (1,32,32,32))\n",
        "print(network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "N4Wi9DatxZFr",
        "outputId": "1467333f-2262-4c1e-814d-ac63194aedde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-da4df6b0ea99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnetwork\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelDefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;31m# make a forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m# remove these hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-1ff8afdc7c24>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1210\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbw_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup_input_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_global_forward_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    612\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 613\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    614\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    606\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m             )\n\u001b[0;32m--> 608\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "cO0v3u39HQj9",
        "outputId": "7ea6d889-ed25-459f-aa31-0f9d040b8dcb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-1a225a187e35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3025\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (64) to match target batch_size (10)."
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "num_models = 1\n",
        "num_samples = X_train_tensor.shape[0]\n",
        "Histories = []\n",
        "model_ = np.empty(num_models, dtype=object)\n",
        "#Repeat 5 times\n",
        "for i in range(num_models):\n",
        "  #Prepare data to be executed\n",
        "  #Extrapolate train data from the bucket\n",
        "  #X_train, y_train = prepare_data(sets[i], \"train\") \n",
        "  #y_train = to_categorical(y_train, 10)\n",
        "  #Shuffle data\n",
        "  #idx = np.random.permutation(len(X_train))\n",
        "  #X_train, y_train = X_train[idx], y_train[idx]\n",
        "  #Apply CUTOUT\n",
        "  #for j in range(int(len(X_train_tensor)*0.25)):\n",
        "  #  X_train[j] = cutout_3d(X_train[j])\n",
        "  #Reapply Shuffle to mix the cutted samples\n",
        "  #idx = np.random.permutation(len(X_train))\n",
        "  #X_train, y_train = X_train[idx], y_train[idx]\n",
        "  #Compile the model\n",
        "  model_[i] = CNN()\n",
        "  # Create a ModelCheckpoint object that saves the model's weights only when the accuracy improves\n",
        "  # I need to re-initialize it every time to save the best model of each iteration\n",
        "  #Save the best model\n",
        "  #with tf.device('/device:GPU:0'):\n",
        "  #checkpoint = torch.utils.ModelCheckpoint('best_model.pth', save_best_only=True, mode='max', verbose=True)\n",
        "  #Early stop callback\n",
        "  #early_stop = torch.nn.EarlyStopping(patience=10, verbose=False, mode='max')\n",
        "  #history = model_[i].fit(X_train, y_train, epochs=30, validation_data=(X_validation, y_validation), callbacks=[ early_stop, checkpoint])\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model_[i].parameters(), lr=0.01)\n",
        "  patience = 10\n",
        "  train_losses = []\n",
        "  val_losses = []\n",
        "  best_acc = 0.0\n",
        "  counter = 0\n",
        "\n",
        "  for epoch in range(30):\n",
        "      # Train\n",
        "      model_[i].train()\n",
        "      train_loss = 0.0\n",
        "      #X_train_tensor = torch.from_numpy(X_train).float()\n",
        "\n",
        "      for inputs, labels in zip(X_train_tensor, y_train_tensor):\n",
        "          #print(inputs.shape)\n",
        "          optimizer.zero_grad()\n",
        "          outputs = model_[i](inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          train_loss += loss.item()\n",
        "\n",
        "      #print(train_loss)\n",
        "      #print(num_samples)\n",
        "      train_losses.append(train_loss / num_samples)\n",
        "\n",
        "      # Validate\n",
        "      model_[i].eval()\n",
        "      val_loss = 0.0\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      with torch.no_grad():\n",
        "          for inputs, labels in zip(X_validation_tensor, y_validation_tensor):\n",
        "              outputs = model_[i](inputs)\n",
        "              #print(\"OUTPUTS\" + str(outputs.shape))\n",
        "              loss = criterion(outputs, labels)\n",
        "              val_loss += loss.item()\n",
        "              _, predicted = torch.max(outputs.data, 0)\n",
        "              total += labels.size(0)\n",
        "              correct += (predicted == labels).sum().item()\n",
        "\n",
        "      val_loss /= num_validation\n",
        "      val_losses.append(val_loss)\n",
        "      acc = 100. * correct / total\n",
        "\n",
        "      if acc > best_acc:\n",
        "          counter = 0\n",
        "          best_acc = acc\n",
        "          torch.save(model_[i].state_dict(), 'best_model.pth')\n",
        "          #early_stop.best = acc\n",
        "      else:\n",
        "        counter = counter + 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping\")\n",
        "            break\n",
        "\n",
        "  print(\"Early Stop called or End of Epochs reaches: Saving the model\")\n",
        "  #Save the model\n",
        "  model_name = \"Model\" + str(i) + \".pth\"\n",
        "  os.rename(\"best_model.h5\", model_name)\n",
        "  #Histories.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V7Lmandt5v3"
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n",
        "  score = model_[i].evaluate(X_validation,y_validation)\n",
        "  print(\"The score of Model \" + str(i) + \" is\")\n",
        "  print(score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_O8dHVZnELv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#plot the train/validation loss and accuracy\n",
        "def plot_graphs(history, metric):\n",
        "    \n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history['val_'+metric], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, 'val_'+metric])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plot_graphs(Histories[0], 'accuracy')\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_graphs(Histories[0], 'loss')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQO2c61Ruc0"
      },
      "source": [
        "# MODEL PREDICTION\n",
        "2 possible ways:\n",
        " - Highest of \"Sum of all the probabilities results\"\n",
        " - Most voted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test = prepare_data(test[0], \"test\")\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "idt = np.random.permutation(len(X_test))\n",
        "X_test, y_test = X_test[idt], y_test[idt]"
      ],
      "metadata": {
        "id": "U8M43xMwbWuc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsP36SY5RwiN"
      },
      "outputs": [],
      "source": [
        "#This function take in input the bagging model and the sample that need to make the prediction\n",
        "#Return the results of the prediction as softmax array\n",
        "def prediction(models,value,true_label):\n",
        "\n",
        "  #Create this to pass the value as input for our models\n",
        "  tensor = []\n",
        "  tensor.append(value)\n",
        "  tensor = np.array(tensor)\n",
        "  #Extrapolate the label from the softmax representation\n",
        "  label = np.argmax(true_label)\n",
        "  \n",
        "  #Doing the prediction on all 5 the models\n",
        "  y = []\n",
        "  for i in range(5):\n",
        "    y.append(models[i].predict(tensor, verbose = 0))\n",
        "  #Sum the results of all the models to obtain the final result\n",
        "  results = np.sum(y,axis=0)  \n",
        "  \n",
        "  max_index = np.unravel_index(np.argmax(results), results.shape)\n",
        "  \n",
        "  #Check if the value was wrong/correct at unanimity\n",
        "  isFinalDecisionCorrect = (label == np.argmax(results))\n",
        "\n",
        "  \n",
        "  if(isFinalDecisionCorrect):\n",
        "    #Final decision is correct, check if \n",
        "    unanimity = True\n",
        "    for i in range(len(models)):\n",
        "      if label != np.argmax(y[i]):\n",
        "        unanimity = False\n",
        "  else:\n",
        "    #Check if it is wrong at unanimity    \n",
        "      unanimity = False\n",
        "      for i in range(len(models)):\n",
        "        if label == np.argmax(y[i]):\n",
        "          unanimity = True\n",
        "  #print(\"True Label: \" + str(label))\n",
        "  #print(\"Predicted label: \" + str(np.argmax(results)))\n",
        "  #Return True if the prediction was correct, and if the prediction was taken at the unanimity\n",
        "\n",
        "  return isFinalDecisionCorrect, unanimity\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56pnNB2lo9S_"
      },
      "outputs": [],
      "source": [
        "prediction(model_,X_test[0],y_test[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVh3p2K0cH6A"
      },
      "outputs": [],
      "source": [
        "#Use our function to evaluate boosting :\n",
        "positive_unanimity = 0\n",
        "positive = 0\n",
        "negative_unanimity = 0\n",
        "negative = 0\n",
        "for i in range(len(X_test)):\n",
        "  isCorrect, unanimity = prediction(model_,X_test[i],y_test[i])\n",
        "  if isCorrect:\n",
        "    if unanimity:\n",
        "      positive_unanimity = positive_unanimity + 1\n",
        "    else:\n",
        "      positive = positive + 1\n",
        "  else:\n",
        "    if unanimity:\n",
        "      negative_unanimity = negative_unanimity + 1\n",
        "    else:\n",
        "      negative = negative + 1\n",
        "\n",
        "print(\"Length of test set: \" + str(len(X_test)))\n",
        "print(\"Positive at unanimity: \" + str(positive_unanimity))\n",
        "print(\"Positive NOT at unanimity: \" + str(positive))\n",
        "print(\"Negative at unanimity: \" + str(negative_unanimity))\n",
        "print(\"Negative NOT at unanimity: \" + str(negative))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X91uj2mQ1uyj"
      },
      "outputs": [],
      "source": [
        "total_positive = positive_unanimity + positive\n",
        "print(total_positive)\n",
        "positive_percentual = total_positive/len(X_test)\n",
        "print(positive_percentual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"Model0.h5\")\n",
        "files.download(\"Model1.h5\")\n",
        "files.download(\"Model2.h5\")\n",
        "files.download(\"Model3.h5\")\n",
        "files.download(\"Model4.h5\")\n"
      ],
      "metadata": {
        "id": "994b7T7UwaVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BjJ9qVoEOPkD"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}