{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y42uYi3LY0aK"
      },
      "source": [
        "# Create a Bagging Model in order to overcome memory space problems\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JXrfj3DjY-j2",
        "outputId": "2e8821c5-f798-4225-f696-b26f8b17b98c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting open3d\n",
            "  Using cached open3d-0.16.0-cp38-cp38-manylinux_2_27_x86_64.whl (422.5 MB)\n",
            "Requirement already satisfied: numpy>1.15 in /usr/local/lib/python3.8/dist-packages (from open3d) (1.21.6)\n",
            "Collecting configargparse\n",
            "  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n",
            "Collecting dash>=2.6.0\n",
            "  Downloading dash-2.8.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.8/dist-packages (from open3d) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.8/dist-packages (from open3d) (1.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from open3d) (4.64.1)\n",
            "Collecting nbformat==5.5.0\n",
            "  Using cached nbformat-5.5.0-py3-none-any.whl (75 kB)\n",
            "Collecting addict\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting pyquaternion\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.8/dist-packages (from open3d) (3.2.2)\n",
            "Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.8/dist-packages (from open3d) (6.0)\n",
            "Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.8/dist-packages (from open3d) (7.7.1)\n",
            "Collecting pillow>=8.2.0\n",
            "  Downloading Pillow-9.4.0-cp38-cp38-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jupyter_core in /usr/local/lib/python3.8/dist-packages (from nbformat==5.5.0->open3d) (5.2.0)\n",
            "Requirement already satisfied: traitlets>=5.1 in /usr/local/lib/python3.8/dist-packages (from nbformat==5.5.0->open3d) (5.7.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat==5.5.0->open3d) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.8/dist-packages (from nbformat==5.5.0->open3d) (2.16.2)\n",
            "Collecting dash-html-components==2.0.0\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.8/dist-packages (from dash>=2.6.0->open3d) (5.5.0)\n",
            "Collecting dash-table==5.0.0\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: Flask>=1.0.4 in /usr/local/lib/python3.8/dist-packages (from dash>=2.6.0->open3d) (1.1.4)\n",
            "Collecting dash-core-components==2.0.0\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.6.0->open3d) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.6.0->open3d) (3.6.1)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.6.0->open3d) (7.9.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.6.0->open3d) (5.3.4)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from ipywidgets>=7.6.0->open3d) (3.0.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->open3d) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib>=3->open3d) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas>=1.0->open3d) (2022.7.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21->open3d) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21->open3d) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug<2.0,>=0.15 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (1.0.1)\n",
            "Requirement already satisfied: Jinja2<3.0,>=2.10.1 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (2.11.3)\n",
            "Requirement already satisfied: itsdangerous<2.0,>=0.24 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (1.1.0)\n",
            "Requirement already satisfied: click<8.0,>=5.1 in /usr/local/lib/python3.8/dist-packages (from Flask>=1.0.4->dash>=2.6.0->open3d) (7.1.2)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (6.1.12)\n",
            "Requirement already satisfied: tornado>=4.2 in /usr/local/lib/python3.8/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (6.0.4)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (4.8.0)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (57.4.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (0.7.5)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (0.2.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (2.6.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (4.4.2)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (2.0.10)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat==5.5.0->open3d) (22.2.0)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat==5.5.0->open3d) (5.10.2)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat==5.5.0->open3d) (0.19.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (1.15.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.8/dist-packages (from plotly>=5.0.0->dash>=2.6.0->open3d) (8.1.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.8/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (5.7.16)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.8/dist-packages (from jupyter_core->nbformat==5.5.0->open3d) (2.6.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.8/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat==5.5.0->open3d) (3.12.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from jedi>=0.10->ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2<3.0,>=2.10.1->Flask>=1.0.4->dash>=2.6.0->open3d) (2.0.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.16.0)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (1.8.0)\n",
            "Requirement already satisfied: nbconvert<6.0 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (5.6.1)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (23.2.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.13.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.8/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (0.2.6)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.8/dist-packages (from pexpect->ipython>=4.0.0->ipywidgets>=7.6.0->open3d) (0.7.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (1.5.0)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.7.1)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (6.0.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.8/dist-packages (from nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.8/dist-packages (from bleach->nbconvert<6.0->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets>=7.6.0->open3d) (0.5.1)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, addict, pyquaternion, pillow, jedi, configargparse, nbformat, dash, open3d\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 7.1.2\n",
            "    Uninstalling Pillow-7.1.2:\n",
            "      Successfully uninstalled Pillow-7.1.2\n",
            "  Attempting uninstall: nbformat\n",
            "    Found existing installation: nbformat 5.7.3\n",
            "    Uninstalling nbformat-5.7.3:\n",
            "      Successfully uninstalled nbformat-5.7.3\n",
            "Successfully installed addict-2.4.0 configargparse-1.5.3 dash-2.8.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 jedi-0.18.2 nbformat-5.5.0 open3d-0.16.0 pillow-9.4.0 pyquaternion-0.9.9\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install open3d\n",
        "import os\n",
        "import open3d\n",
        "from random import sample\n",
        "import numpy as np\n",
        "from keras.layers import Conv3D, MaxPooling3D, Flatten, Dense\n",
        "from keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8kKhTK4NY5T"
      },
      "source": [
        "# MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VARhfPYfYxXe"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Os30XLZHY-wN"
      },
      "outputs": [],
      "source": [
        "def model_definition():\n",
        "  # create model\n",
        "  model = Sequential()\n",
        "\n",
        "  # add convolutional layers\n",
        "  model.add(Conv3D(16, kernel_size=(3, 3, 3), input_shape=(32, 32, 32, 1), activation='relu'))\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "  model.add(Conv3D(32, kernel_size=(3, 3, 3), activation='relu'))\n",
        "  model.add(MaxPooling3D(pool_size=(3, 3, 3)))\n",
        "\n",
        "  # add flatten layer\n",
        "  model.add(Flatten())\n",
        "\n",
        "  # add dense layers\n",
        "  model.add(Dense(64, activation='relu'))\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "  # compile model\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbRo3Gh2NZ3c"
      },
      "source": [
        "# DATA PREPARATION:\n",
        "Splitting the train set into 5 set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGVDHGlPTbRc"
      },
      "outputs": [],
      "source": [
        "#Extrapolate the voxels from the file \n",
        "def process_off_file(filepath):\n",
        "\n",
        "    voxel = open3d.io.read_voxel_grid(filepath)\n",
        "    return voxel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T1qsMWQ5Y-5F"
      },
      "outputs": [],
      "source": [
        "def getKPartitionsFolderized(folderName, K, folderTest):\n",
        "    \n",
        "    # dooing for all the models\n",
        "    baseDIR = os.path.dirname(\"/content/drive/MyDrive/\")\n",
        "    rootModelsDirName = os.path.join(baseDIR, folderName)\n",
        "\n",
        "    models = [\"bathtub\", \"bed\", \"chair\", \"desk\", \"dresser\",\n",
        "              \"monitor\", \"night_stand\", \"sofa\", \"table\", \"toilet\"]\n",
        "    # models = [\"desk\"]\n",
        "\n",
        "    # PUSHING ALL models in allModels\n",
        "    allModels = {}\n",
        "\n",
        "    for modelFolder in models:\n",
        "        # print(f'current modelFolderName= {modelFolder} and isTestFolder={isTestModel}')\n",
        "        # PRELIMINARY STEPS for getting the input folder and creating respective output folder\n",
        "\n",
        "        # 1) Getting INPUT FILES\n",
        "        inputDIR = os.path.join(\n",
        "            rootModelsDirName, modelFolder, \"test\" if folderTest else \"train\")\n",
        "        #print(f'Working on {modelFolder} in folder {inputDIR}')\n",
        "        #print(os.path.isdir(inputDIR))\n",
        "\n",
        "        # Getting the list of all mesh in the directory 'modelFolder'\n",
        "        inputModels = []\n",
        "        INPUT_EXTENTION = \".ply\"\n",
        "        # Iterate directory\n",
        "        for path in os.listdir(inputDIR):\n",
        "            # check if current path is an expected file\n",
        "            if os.path.isfile(os.path.join(inputDIR, path)) and os.path.join(inputDIR, path).endswith(INPUT_EXTENTION):\n",
        "                # append only the file name\n",
        "                inputModels.append(os.path.splitext(path)[0])\n",
        "        # print(2, inputModels)\n",
        "        allModels[modelFolder] = inputModels\n",
        "\n",
        "    # DIVIDING allModels into K Partitions\n",
        "    k_sets_indexes = [i for i in range(0, K)]\n",
        "    randomsGlobal = [0 for _ in range(0, K)]\n",
        "\n",
        "    # array di K dictionaries inizializzati a empty\n",
        "    k_sets = [{} for _ in range(0, K)]\n",
        "    for key in allModels:\n",
        "        #print(key, len(allModels[key]))\n",
        "        for v in allModels[key]:\n",
        "            # of the bucket\n",
        "            index = sample(k_sets_indexes, 1)[0]\n",
        "            if not k_sets[index].keys().__contains__(key):\n",
        "                k_sets[index][key] = []\n",
        "            k_sets[index][key].append(v)\n",
        "            # print(index)\n",
        "            randomsGlobal[index] += 1\n",
        "\n",
        "    #print(f\"Randoms: {randomsGlobal}\")\n",
        "    return k_sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68hkTyBDS3ml"
      },
      "source": [
        "# Prepare validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i6nuwE-OquBL"
      },
      "outputs": [],
      "source": [
        "#From a set create the train set\n",
        "def prepare_data(partition, middlefolder):\n",
        "\n",
        "  train = []\n",
        "  labels = []\n",
        "\n",
        "  #Read all the data from a single partition of the dataset\n",
        "  #Create the filepath for each \n",
        "  train_data_raw = []  \n",
        "  i = -1\n",
        "  ordered_keys = sorted(partition.keys())\n",
        "  for key in ordered_keys:\n",
        "    i = i + 1\n",
        "    for value in partition[key]:\n",
        "      filepath = f'/content/drive/MyDrive/Output_ROTATED_v7/{key}/{middlefolder}/{value}.ply'\n",
        "      points = process_off_file(filepath)\n",
        "      train_data_raw.append(points)\n",
        "      labels.append(i)      \n",
        "\n",
        "  # Transform the Voxel Grid into Numpy Array containing a list of voxels \n",
        "  train_data_numpy = []\n",
        "  \n",
        "  #First step: Each array to voxels \n",
        "  for i in range(len(train_data_raw)):\n",
        "    train_data_numpy.append(np.asarray(train_data_raw[i].get_voxels()))\n",
        "\n",
        "  array_train_ready = []\n",
        "  array_train_normalized = []\n",
        "  \n",
        "  #Create the compact 32x32x32 vector\n",
        "  for i in range(len(train_data_numpy)):\n",
        "    array_temp = np.zeros((32, 32, 32,1))\n",
        "    for j in range(len(train_data_numpy[i])):\n",
        "        array_temp[train_data_numpy[i][j].grid_index[0],train_data_numpy[i][j].grid_index[1],train_data_numpy[i][j].grid_index[2]] = 1\n",
        "    #Perform normalization\n",
        "    #mean = np.mean(array_temp, axis = 0)\n",
        "    #std = np.std(array_temp, axis = 0)\n",
        "    #array_train_ready.append((array_temp - mean)/std)\n",
        "    array_train_ready.append(array_temp)\n",
        "\n",
        "  array_train_ready = np.asarray(array_train_ready) \n",
        "  train = np.array(array_train_ready)\n",
        "\n",
        "  #From list to array\n",
        "  labels = np.array(labels)\n",
        "\n",
        "  return train,labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIYbeKFsNlao"
      },
      "outputs": [],
      "source": [
        "#Create a unique validation set for every model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg96kzP3Y_Hm"
      },
      "source": [
        "# MODEL PREPARATION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xXBxUwSZcdk",
        "outputId": "3cf26c45-728b-4c80-bf31-67dda592216c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['bathtub', 'bed', 'chair', 'desk', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n"
          ]
        }
      ],
      "source": [
        "#Create now several models using the train/validation data\n",
        "\n",
        "#Print Labels \n",
        "class_folders = os.listdir('/content/drive/MyDrive/Output_ROTATED_v7')\n",
        "print(class_folders)\n",
        "\n",
        "#Prepare DATA\n",
        "folder = 'Output_ROTATED_v7'\n",
        "\n",
        "#SPLIT THE TRAIN SET\n",
        "#Sets will contain an array of 5 elements, containing 5 partition of the Train set\n",
        "#Each of these partition is divided thanks to a map, that contains:\n",
        "#Key: the name of the class, Value: an array of path\n",
        "\n",
        "sets = getKPartitionsFolderized(folder, 5, False)\n",
        "\n",
        "#Split the test set, in order to create a Validation Set and a Test set\n",
        "test = getKPartitionsFolderized(folder,2,True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kaYf4IZULYA"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "X_validation, y_validation = prepare_data(test[1], \"test\")\n",
        "\n",
        "X_test, y_test = prepare_data(test[0], \"test\")\n",
        "\n",
        "y_test = to_categorical(y_test, 10)\n",
        "y_validation = to_categorical(y_validation, 10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MeAuq2BatZkl"
      },
      "outputs": [],
      "source": [
        "def cutout_3d(volume, cutout_size=(8,8,8)):\n",
        "    # Shape is 32x32x32\n",
        "    x = np.random.randint(0, 32 - cutout_size[0])\n",
        "    y = np.random.randint(0, 32 - cutout_size[1])\n",
        "    z = np.random.randint(0, 32 - cutout_size[2])\n",
        "    cutout_cube = (x, y, z, cutout_size[0], cutout_size[1], cutout_size[2])\n",
        "    volume[x:x+cutout_size[0], y:y+cutout_size[1], z:z+cutout_size[2]] = 0\n",
        "    return volume"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmlNdQPruk1K"
      },
      "outputs": [],
      "source": [
        "idt = np.random.permutation(len(X_test))\n",
        "X_test, y_test = X_test[idt], y_test[idt]\n",
        "\n",
        "idt = np.random.permutation(len(X_validation))\n",
        "X_validation, y_validation = X_validation[idt], y_validation[idt]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO0v3u39HQj9",
        "outputId": "ec4d13f8-5d2f-4b47-da2f-9c449bcdd984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.7199 - accuracy: 0.3683\n",
            "Epoch 1: val_loss improved from inf to 1.29753, saving model to best_model.h5\n",
            "63/63 [==============================] - 176s 3s/step - loss: 1.7199 - accuracy: 0.3683 - val_loss: 1.2975 - val_accuracy: 0.5382\n",
            "Epoch 2/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 1.1003 - accuracy: 0.6207\n",
            "Epoch 2: val_loss improved from 1.29753 to 1.04527, saving model to best_model.h5\n",
            "63/63 [==============================] - 174s 3s/step - loss: 1.1003 - accuracy: 0.6207 - val_loss: 1.0453 - val_accuracy: 0.6304\n",
            "Epoch 3/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.8851 - accuracy: 0.7011\n",
            "Epoch 3: val_loss improved from 1.04527 to 0.92732, saving model to best_model.h5\n",
            "63/63 [==============================] - 177s 3s/step - loss: 0.8851 - accuracy: 0.7011 - val_loss: 0.9273 - val_accuracy: 0.6729\n",
            "Epoch 4/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7437 - accuracy: 0.7511\n",
            "Epoch 4: val_loss improved from 0.92732 to 0.81240, saving model to best_model.h5\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.7437 - accuracy: 0.7511 - val_loss: 0.8124 - val_accuracy: 0.7277\n",
            "Epoch 5/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.7092 - accuracy: 0.7641\n",
            "Epoch 5: val_loss did not improve from 0.81240\n",
            "63/63 [==============================] - 173s 3s/step - loss: 0.7092 - accuracy: 0.7641 - val_loss: 0.8630 - val_accuracy: 0.7161\n",
            "Epoch 6/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.6257 - accuracy: 0.7866\n",
            "Epoch 6: val_loss improved from 0.81240 to 0.76848, saving model to best_model.h5\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.6257 - accuracy: 0.7866 - val_loss: 0.7685 - val_accuracy: 0.7428\n",
            "Epoch 7/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.8091\n",
            "Epoch 7: val_loss did not improve from 0.76848\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.5395 - accuracy: 0.8091 - val_loss: 0.7964 - val_accuracy: 0.7298\n",
            "Epoch 8/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.5060 - accuracy: 0.8261\n",
            "Epoch 8: val_loss did not improve from 0.76848\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.5060 - accuracy: 0.8261 - val_loss: 0.8164 - val_accuracy: 0.7334\n",
            "Epoch 9/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4856 - accuracy: 0.8256\n",
            "Epoch 9: val_loss improved from 0.76848 to 0.69778, saving model to best_model.h5\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.4856 - accuracy: 0.8256 - val_loss: 0.6978 - val_accuracy: 0.7601\n",
            "Epoch 10/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.4226 - accuracy: 0.8441\n",
            "Epoch 10: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.4226 - accuracy: 0.8441 - val_loss: 0.7832 - val_accuracy: 0.7269\n",
            "Epoch 11/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3732 - accuracy: 0.8716\n",
            "Epoch 11: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.3732 - accuracy: 0.8716 - val_loss: 0.7627 - val_accuracy: 0.7486\n",
            "Epoch 12/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3406 - accuracy: 0.8776\n",
            "Epoch 12: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 174s 3s/step - loss: 0.3406 - accuracy: 0.8776 - val_loss: 0.7934 - val_accuracy: 0.7378\n",
            "Epoch 13/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3121 - accuracy: 0.8881\n",
            "Epoch 13: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.3121 - accuracy: 0.8881 - val_loss: 0.8431 - val_accuracy: 0.7414\n",
            "Epoch 14/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.3002 - accuracy: 0.8946\n",
            "Epoch 14: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 173s 3s/step - loss: 0.3002 - accuracy: 0.8946 - val_loss: 1.0772 - val_accuracy: 0.6895\n",
            "Epoch 15/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2852 - accuracy: 0.8876\n",
            "Epoch 15: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 178s 3s/step - loss: 0.2852 - accuracy: 0.8876 - val_loss: 0.8132 - val_accuracy: 0.7522\n",
            "Epoch 16/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.2134 - accuracy: 0.9240\n",
            "Epoch 16: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 176s 3s/step - loss: 0.2134 - accuracy: 0.9240 - val_loss: 0.8285 - val_accuracy: 0.7644\n",
            "Epoch 17/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1748 - accuracy: 0.9435\n",
            "Epoch 17: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 173s 3s/step - loss: 0.1748 - accuracy: 0.9435 - val_loss: 0.9337 - val_accuracy: 0.7378\n",
            "Epoch 18/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1751 - accuracy: 0.9355\n",
            "Epoch 18: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.1751 - accuracy: 0.9355 - val_loss: 0.8868 - val_accuracy: 0.7550\n",
            "Epoch 19/30\n",
            "63/63 [==============================] - ETA: 0s - loss: 0.1315 - accuracy: 0.9610\n",
            "Epoch 19: val_loss did not improve from 0.69778\n",
            "63/63 [==============================] - 175s 3s/step - loss: 0.1315 - accuracy: 0.9610 - val_loss: 0.8849 - val_accuracy: 0.7630\n",
            "Early Stop called or End of Epochs reaches: Saving the model\n",
            "Epoch 1/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 1.7305 - accuracy: 0.3825\n",
            "Epoch 1: val_loss improved from inf to 1.23807, saving model to best_model.h5\n",
            "64/64 [==============================] - 178s 3s/step - loss: 1.7305 - accuracy: 0.3825 - val_loss: 1.2381 - val_accuracy: 0.5713\n",
            "Epoch 2/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 1.0363 - accuracy: 0.6358\n",
            "Epoch 2: val_loss improved from 1.23807 to 1.01233, saving model to best_model.h5\n",
            "64/64 [==============================] - 175s 3s/step - loss: 1.0363 - accuracy: 0.6358 - val_loss: 1.0123 - val_accuracy: 0.6326\n",
            "Epoch 3/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.8347 - accuracy: 0.7132\n",
            "Epoch 3: val_loss improved from 1.01233 to 0.97302, saving model to best_model.h5\n",
            "64/64 [==============================] - 174s 3s/step - loss: 0.8347 - accuracy: 0.7132 - val_loss: 0.9730 - val_accuracy: 0.6679\n",
            "Epoch 4/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.7210 - accuracy: 0.7482\n",
            "Epoch 4: val_loss improved from 0.97302 to 0.80061, saving model to best_model.h5\n",
            "64/64 [==============================] - 177s 3s/step - loss: 0.7210 - accuracy: 0.7482 - val_loss: 0.8006 - val_accuracy: 0.7219\n",
            "Epoch 5/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.6236 - accuracy: 0.7896\n",
            "Epoch 5: val_loss improved from 0.80061 to 0.74231, saving model to best_model.h5\n",
            "64/64 [==============================] - 175s 3s/step - loss: 0.6236 - accuracy: 0.7896 - val_loss: 0.7423 - val_accuracy: 0.7435\n",
            "Epoch 6/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.8098\n",
            "Epoch 6: val_loss did not improve from 0.74231\n",
            "64/64 [==============================] - 173s 3s/step - loss: 0.5528 - accuracy: 0.8098 - val_loss: 0.7450 - val_accuracy: 0.7284\n",
            "Epoch 7/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4742 - accuracy: 0.8339\n",
            "Epoch 7: val_loss improved from 0.74231 to 0.71317, saving model to best_model.h5\n",
            "64/64 [==============================] - 176s 3s/step - loss: 0.4742 - accuracy: 0.8339 - val_loss: 0.7132 - val_accuracy: 0.7637\n",
            "Epoch 8/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4303 - accuracy: 0.8492\n",
            "Epoch 8: val_loss did not improve from 0.71317\n",
            "64/64 [==============================] - 178s 3s/step - loss: 0.4303 - accuracy: 0.8492 - val_loss: 0.7611 - val_accuracy: 0.7334\n",
            "Epoch 9/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.4022 - accuracy: 0.8610\n",
            "Epoch 9: val_loss did not improve from 0.71317\n",
            "64/64 [==============================] - 176s 3s/step - loss: 0.4022 - accuracy: 0.8610 - val_loss: 0.7475 - val_accuracy: 0.7637\n",
            "Epoch 10/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.3245 - accuracy: 0.8901\n",
            "Epoch 10: val_loss improved from 0.71317 to 0.70614, saving model to best_model.h5\n",
            "64/64 [==============================] - 173s 3s/step - loss: 0.3245 - accuracy: 0.8901 - val_loss: 0.7061 - val_accuracy: 0.7687\n",
            "Epoch 11/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2839 - accuracy: 0.9098\n",
            "Epoch 11: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 179s 3s/step - loss: 0.2839 - accuracy: 0.9098 - val_loss: 0.7443 - val_accuracy: 0.7565\n",
            "Epoch 12/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2647 - accuracy: 0.9049\n",
            "Epoch 12: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 177s 3s/step - loss: 0.2647 - accuracy: 0.9049 - val_loss: 0.8077 - val_accuracy: 0.7550\n",
            "Epoch 13/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2298 - accuracy: 0.9187\n",
            "Epoch 13: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 173s 3s/step - loss: 0.2298 - accuracy: 0.9187 - val_loss: 0.8062 - val_accuracy: 0.7442\n",
            "Epoch 14/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9197\n",
            "Epoch 14: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 175s 3s/step - loss: 0.2291 - accuracy: 0.9197 - val_loss: 0.7735 - val_accuracy: 0.7558\n",
            "Epoch 15/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1896 - accuracy: 0.9290\n",
            "Epoch 15: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 178s 3s/step - loss: 0.1896 - accuracy: 0.9290 - val_loss: 0.9195 - val_accuracy: 0.7500\n",
            "Epoch 16/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1901 - accuracy: 0.9325\n",
            "Epoch 16: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 174s 3s/step - loss: 0.1901 - accuracy: 0.9325 - val_loss: 0.8290 - val_accuracy: 0.7702\n",
            "Epoch 17/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1500 - accuracy: 0.9478\n",
            "Epoch 17: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 175s 3s/step - loss: 0.1500 - accuracy: 0.9478 - val_loss: 0.8542 - val_accuracy: 0.7601\n",
            "Epoch 18/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.1196 - accuracy: 0.9630\n",
            "Epoch 18: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 175s 3s/step - loss: 0.1196 - accuracy: 0.9630 - val_loss: 0.8761 - val_accuracy: 0.7709\n",
            "Epoch 19/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0972 - accuracy: 0.9709\n",
            "Epoch 19: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 178s 3s/step - loss: 0.0972 - accuracy: 0.9709 - val_loss: 0.8582 - val_accuracy: 0.7687\n",
            "Epoch 20/30\n",
            "64/64 [==============================] - ETA: 0s - loss: 0.0819 - accuracy: 0.9763\n",
            "Epoch 20: val_loss did not improve from 0.70614\n",
            "64/64 [==============================] - 174s 3s/step - loss: 0.0819 - accuracy: 0.9763 - val_loss: 0.8737 - val_accuracy: 0.7716\n",
            "Early Stop called or End of Epochs reaches: Saving the model\n",
            "Epoch 1/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 1.7987 - accuracy: 0.3369\n",
            "Epoch 1: val_loss improved from inf to 1.37949, saving model to best_model.h5\n",
            "62/62 [==============================] - 171s 3s/step - loss: 1.7987 - accuracy: 0.3369 - val_loss: 1.3795 - val_accuracy: 0.4942\n",
            "Epoch 2/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 1.1714 - accuracy: 0.5945\n",
            "Epoch 2: val_loss improved from 1.37949 to 1.14175, saving model to best_model.h5\n",
            "62/62 [==============================] - 173s 3s/step - loss: 1.1714 - accuracy: 0.5945 - val_loss: 1.1417 - val_accuracy: 0.6282\n",
            "Epoch 3/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.8987 - accuracy: 0.6936\n",
            "Epoch 3: val_loss improved from 1.14175 to 0.99854, saving model to best_model.h5\n",
            "62/62 [==============================] - 172s 3s/step - loss: 0.8987 - accuracy: 0.6936 - val_loss: 0.9985 - val_accuracy: 0.6765\n",
            "Epoch 4/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.7408 - accuracy: 0.7424\n",
            "Epoch 4: val_loss did not improve from 0.99854\n",
            "62/62 [==============================] - 170s 3s/step - loss: 0.7408 - accuracy: 0.7424 - val_loss: 1.0997 - val_accuracy: 0.6362\n",
            "Epoch 5/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.6607 - accuracy: 0.7698\n",
            "Epoch 5: val_loss improved from 0.99854 to 0.99821, saving model to best_model.h5\n",
            "62/62 [==============================] - 172s 3s/step - loss: 0.6607 - accuracy: 0.7698 - val_loss: 0.9982 - val_accuracy: 0.6722\n",
            "Epoch 6/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.5461 - accuracy: 0.8181\n",
            "Epoch 6: val_loss improved from 0.99821 to 0.81748, saving model to best_model.h5\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.5461 - accuracy: 0.8181 - val_loss: 0.8175 - val_accuracy: 0.7385\n",
            "Epoch 7/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.4825 - accuracy: 0.8338\n",
            "Epoch 7: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.4825 - accuracy: 0.8338 - val_loss: 0.9238 - val_accuracy: 0.6988\n",
            "Epoch 8/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.4052 - accuracy: 0.8638\n",
            "Epoch 8: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 170s 3s/step - loss: 0.4052 - accuracy: 0.8638 - val_loss: 0.8761 - val_accuracy: 0.7327\n",
            "Epoch 9/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.3639 - accuracy: 0.8770\n",
            "Epoch 9: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.3639 - accuracy: 0.8770 - val_loss: 0.9677 - val_accuracy: 0.7291\n",
            "Epoch 10/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.3222 - accuracy: 0.8928\n",
            "Epoch 10: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.3222 - accuracy: 0.8928 - val_loss: 0.8905 - val_accuracy: 0.7450\n",
            "Epoch 11/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.2733 - accuracy: 0.9029\n",
            "Epoch 11: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 171s 3s/step - loss: 0.2733 - accuracy: 0.9029 - val_loss: 0.8930 - val_accuracy: 0.7442\n",
            "Epoch 12/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.2249 - accuracy: 0.9217\n",
            "Epoch 12: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.2249 - accuracy: 0.9217 - val_loss: 1.1355 - val_accuracy: 0.7003\n",
            "Epoch 13/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.2459 - accuracy: 0.9162\n",
            "Epoch 13: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 174s 3s/step - loss: 0.2459 - accuracy: 0.9162 - val_loss: 0.9386 - val_accuracy: 0.7615\n",
            "Epoch 14/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.1756 - accuracy: 0.9466\n",
            "Epoch 14: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 171s 3s/step - loss: 0.1756 - accuracy: 0.9466 - val_loss: 1.0981 - val_accuracy: 0.7284\n",
            "Epoch 15/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9533\n",
            "Epoch 15: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 171s 3s/step - loss: 0.1397 - accuracy: 0.9533 - val_loss: 1.0857 - val_accuracy: 0.7406\n",
            "Epoch 16/30\n",
            "62/62 [==============================] - ETA: 0s - loss: 0.1512 - accuracy: 0.9436\n",
            "Epoch 16: val_loss did not improve from 0.81748\n",
            "62/62 [==============================] - 173s 3s/step - loss: 0.1512 - accuracy: 0.9436 - val_loss: 1.1523 - val_accuracy: 0.7392\n",
            "Early Stop called or End of Epochs reaches: Saving the model\n",
            "Epoch 1/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 1.7634 - accuracy: 0.3647\n",
            "Epoch 1: val_loss improved from inf to 1.33661, saving model to best_model.h5\n",
            "65/65 [==============================] - 180s 3s/step - loss: 1.7634 - accuracy: 0.3647 - val_loss: 1.3366 - val_accuracy: 0.5483\n",
            "Epoch 2/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 1.0313 - accuracy: 0.6319\n",
            "Epoch 2: val_loss improved from 1.33661 to 1.00728, saving model to best_model.h5\n",
            "65/65 [==============================] - 179s 3s/step - loss: 1.0313 - accuracy: 0.6319 - val_loss: 1.0073 - val_accuracy: 0.6585\n",
            "Epoch 3/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.8024 - accuracy: 0.7289\n",
            "Epoch 3: val_loss improved from 1.00728 to 0.93486, saving model to best_model.h5\n",
            "65/65 [==============================] - 178s 3s/step - loss: 0.8024 - accuracy: 0.7289 - val_loss: 0.9349 - val_accuracy: 0.6643\n",
            "Epoch 4/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.7365 - accuracy: 0.7410\n",
            "Epoch 4: val_loss improved from 0.93486 to 0.85552, saving model to best_model.h5\n",
            "65/65 [==============================] - 181s 3s/step - loss: 0.7365 - accuracy: 0.7410 - val_loss: 0.8555 - val_accuracy: 0.7075\n",
            "Epoch 5/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.5835 - accuracy: 0.8016\n",
            "Epoch 5: val_loss improved from 0.85552 to 0.82930, saving model to best_model.h5\n",
            "65/65 [==============================] - 177s 3s/step - loss: 0.5835 - accuracy: 0.8016 - val_loss: 0.8293 - val_accuracy: 0.7219\n",
            "Epoch 6/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.8278\n",
            "Epoch 6: val_loss did not improve from 0.82930\n",
            "65/65 [==============================] - 181s 3s/step - loss: 0.5243 - accuracy: 0.8278 - val_loss: 0.8846 - val_accuracy: 0.7241\n",
            "Epoch 7/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.4765 - accuracy: 0.8283\n",
            "Epoch 7: val_loss improved from 0.82930 to 0.79130, saving model to best_model.h5\n",
            "65/65 [==============================] - 176s 3s/step - loss: 0.4765 - accuracy: 0.8283 - val_loss: 0.7913 - val_accuracy: 0.7327\n",
            "Epoch 8/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.4010 - accuracy: 0.8598\n",
            "Epoch 8: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 179s 3s/step - loss: 0.4010 - accuracy: 0.8598 - val_loss: 1.0649 - val_accuracy: 0.7061\n",
            "Epoch 9/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.3770 - accuracy: 0.8720\n",
            "Epoch 9: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 180s 3s/step - loss: 0.3770 - accuracy: 0.8720 - val_loss: 0.8510 - val_accuracy: 0.7305\n",
            "Epoch 10/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.3400 - accuracy: 0.8802\n",
            "Epoch 10: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 176s 3s/step - loss: 0.3400 - accuracy: 0.8802 - val_loss: 0.9034 - val_accuracy: 0.7233\n",
            "Epoch 11/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.3047 - accuracy: 0.8943\n",
            "Epoch 11: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 181s 3s/step - loss: 0.3047 - accuracy: 0.8943 - val_loss: 0.8314 - val_accuracy: 0.7464\n",
            "Epoch 12/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.2650 - accuracy: 0.9030\n",
            "Epoch 12: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 178s 3s/step - loss: 0.2650 - accuracy: 0.9030 - val_loss: 0.8752 - val_accuracy: 0.7406\n",
            "Epoch 13/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.2310 - accuracy: 0.9214\n",
            "Epoch 13: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 178s 3s/step - loss: 0.2310 - accuracy: 0.9214 - val_loss: 0.9114 - val_accuracy: 0.7493\n",
            "Epoch 14/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.1848 - accuracy: 0.9418\n",
            "Epoch 14: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 178s 3s/step - loss: 0.1848 - accuracy: 0.9418 - val_loss: 0.9362 - val_accuracy: 0.7543\n",
            "Epoch 15/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.1731 - accuracy: 0.9321\n",
            "Epoch 15: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 181s 3s/step - loss: 0.1731 - accuracy: 0.9321 - val_loss: 0.9568 - val_accuracy: 0.7514\n",
            "Epoch 16/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.1719 - accuracy: 0.9428\n",
            "Epoch 16: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 177s 3s/step - loss: 0.1719 - accuracy: 0.9428 - val_loss: 0.9493 - val_accuracy: 0.7514\n",
            "Epoch 17/30\n",
            "65/65 [==============================] - ETA: 0s - loss: 0.1381 - accuracy: 0.9510\n",
            "Epoch 17: val_loss did not improve from 0.79130\n",
            "65/65 [==============================] - 179s 3s/step - loss: 0.1381 - accuracy: 0.9510 - val_loss: 1.0726 - val_accuracy: 0.7334\n",
            "Early Stop called or End of Epochs reaches: Saving the model\n",
            "Epoch 1/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.7726 - accuracy: 0.3681\n",
            "Epoch 1: val_loss improved from inf to 1.33880, saving model to best_model.h5\n",
            "61/61 [==============================] - 169s 3s/step - loss: 1.7726 - accuracy: 0.3681 - val_loss: 1.3388 - val_accuracy: 0.4769\n",
            "Epoch 2/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 1.1593 - accuracy: 0.6158\n",
            "Epoch 2: val_loss improved from 1.33880 to 1.07881, saving model to best_model.h5\n",
            "61/61 [==============================] - 167s 3s/step - loss: 1.1593 - accuracy: 0.6158 - val_loss: 1.0788 - val_accuracy: 0.6066\n",
            "Epoch 3/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.8873 - accuracy: 0.7087\n",
            "Epoch 3: val_loss improved from 1.07881 to 0.90085, saving model to best_model.h5\n",
            "61/61 [==============================] - 169s 3s/step - loss: 0.8873 - accuracy: 0.7087 - val_loss: 0.9008 - val_accuracy: 0.6722\n",
            "Epoch 4/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.7317 - accuracy: 0.7684\n",
            "Epoch 4: val_loss did not improve from 0.90085\n",
            "61/61 [==============================] - 170s 3s/step - loss: 0.7317 - accuracy: 0.7684 - val_loss: 0.9022 - val_accuracy: 0.6888\n",
            "Epoch 5/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.6281 - accuracy: 0.7866\n",
            "Epoch 5: val_loss did not improve from 0.90085\n",
            "61/61 [==============================] - 168s 3s/step - loss: 0.6281 - accuracy: 0.7866 - val_loss: 0.9286 - val_accuracy: 0.6931\n",
            "Epoch 6/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5499 - accuracy: 0.8178\n",
            "Epoch 6: val_loss improved from 0.90085 to 0.85715, saving model to best_model.h5\n",
            "61/61 [==============================] - 166s 3s/step - loss: 0.5499 - accuracy: 0.8178 - val_loss: 0.8572 - val_accuracy: 0.7197\n",
            "Epoch 7/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.5235 - accuracy: 0.8198\n",
            "Epoch 7: val_loss did not improve from 0.85715\n",
            "61/61 [==============================] - 166s 3s/step - loss: 0.5235 - accuracy: 0.8198 - val_loss: 0.9961 - val_accuracy: 0.6880\n",
            "Epoch 8/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.4636 - accuracy: 0.8422\n",
            "Epoch 8: val_loss did not improve from 0.85715\n",
            "61/61 [==============================] - 168s 3s/step - loss: 0.4636 - accuracy: 0.8422 - val_loss: 0.8804 - val_accuracy: 0.7241\n",
            "Epoch 9/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3824 - accuracy: 0.8686\n",
            "Epoch 9: val_loss improved from 0.85715 to 0.85637, saving model to best_model.h5\n",
            "61/61 [==============================] - 167s 3s/step - loss: 0.3824 - accuracy: 0.8686 - val_loss: 0.8564 - val_accuracy: 0.7486\n",
            "Epoch 10/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3395 - accuracy: 0.8816\n",
            "Epoch 10: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 168s 3s/step - loss: 0.3395 - accuracy: 0.8816 - val_loss: 0.9371 - val_accuracy: 0.7305\n",
            "Epoch 11/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3191 - accuracy: 0.8925\n",
            "Epoch 11: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 167s 3s/step - loss: 0.3191 - accuracy: 0.8925 - val_loss: 0.8672 - val_accuracy: 0.7370\n",
            "Epoch 12/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.3602 - accuracy: 0.8733\n",
            "Epoch 12: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 166s 3s/step - loss: 0.3602 - accuracy: 0.8733 - val_loss: 1.0194 - val_accuracy: 0.7197\n",
            "Epoch 13/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.2350 - accuracy: 0.9143\n",
            "Epoch 13: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 169s 3s/step - loss: 0.2350 - accuracy: 0.9143 - val_loss: 1.0192 - val_accuracy: 0.7320\n",
            "Epoch 14/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.2241 - accuracy: 0.9206\n",
            "Epoch 14: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 167s 3s/step - loss: 0.2241 - accuracy: 0.9206 - val_loss: 0.9573 - val_accuracy: 0.7464\n",
            "Epoch 15/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1694 - accuracy: 0.9439\n",
            "Epoch 15: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 168s 3s/step - loss: 0.1694 - accuracy: 0.9439 - val_loss: 1.1034 - val_accuracy: 0.7176\n",
            "Epoch 16/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1773 - accuracy: 0.9356\n",
            "Epoch 16: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 168s 3s/step - loss: 0.1773 - accuracy: 0.9356 - val_loss: 1.0784 - val_accuracy: 0.7298\n",
            "Epoch 17/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1217 - accuracy: 0.9631\n",
            "Epoch 17: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 166s 3s/step - loss: 0.1217 - accuracy: 0.9631 - val_loss: 1.2352 - val_accuracy: 0.7219\n",
            "Epoch 18/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1131 - accuracy: 0.9678\n",
            "Epoch 18: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 167s 3s/step - loss: 0.1131 - accuracy: 0.9678 - val_loss: 1.1672 - val_accuracy: 0.7385\n",
            "Epoch 19/30\n",
            "61/61 [==============================] - ETA: 0s - loss: 0.1397 - accuracy: 0.9522\n",
            "Epoch 19: val_loss did not improve from 0.85637\n",
            "61/61 [==============================] - 167s 3s/step - loss: 0.1397 - accuracy: 0.9522 - val_loss: 1.1889 - val_accuracy: 0.7457\n",
            "Early Stop called or End of Epochs reaches: Saving the model\n"
          ]
        }
      ],
      "source": [
        "Histories = []\n",
        "#Create Early stop and 30 epochs\n",
        "\n",
        "model_ = np.empty(5, dtype=object)\n",
        "#Repeat 5 times\n",
        "for i in range(5):\n",
        "  #Prepare data to be executed\n",
        "  #Extrapolate train data from the bucket\n",
        "  X_train, y_train = prepare_data(sets[i], \"train\") \n",
        "  y_train = to_categorical(y_train, 10)\n",
        "  #Shuffle data\n",
        "  idx = np.random.permutation(len(X_train))\n",
        "  X_train, y_train = X_train[idx], y_train[idx]\n",
        "  #Apply CUTOUT\n",
        "  for j in range(int(len(X_train)*0.25)):\n",
        "    X_train[j] = cutout_3d(X_train[j])\n",
        "  #Reapply Shuffle to mix the cutted samples\n",
        "  idx = np.random.permutation(len(X_train))\n",
        "  X_train, y_train = X_train[idx], y_train[idx]\n",
        "  #Compile the model\n",
        "  model_[i] = model_definition()\n",
        "  # Create a ModelCheckpoint object that saves the model's weights only when the accuracy improves\n",
        "  # I need to re-initialize it every time to save the best model of each iteration\n",
        "  #Save the best model\n",
        "  checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_loss', mode='min', verbose=1)\n",
        "  #Early stop callback\n",
        "  early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "  history = model_[i].fit(X_train, y_train, epochs=30, validation_data=(X_validation, y_validation), callbacks=[checkpoint, early_stop])\n",
        "  print(\"Early Stop called or End of Epochs reaches: Saving the model\")\n",
        "  #Save the model\n",
        "  model_name = \"Model\" + str(i) + \".h5\"\n",
        "  os.rename(\"best_model.h5\", model_name)\n",
        "  Histories.append(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2V7Lmandt5v3",
        "outputId": "cae94a57-66af-4b71-b493-80ae842550f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 22s 493ms/step - loss: 0.6978 - accuracy: 0.7601\n",
            "The score of Model 0 is\n",
            "0.7600864768028259\n",
            "44/44 [==============================] - 24s 557ms/step - loss: 0.7061 - accuracy: 0.7687\n",
            "The score of Model 1 is\n",
            "0.7687320113182068\n",
            "44/44 [==============================] - 22s 491ms/step - loss: 0.8175 - accuracy: 0.7385\n",
            "The score of Model 2 is\n",
            "0.7384726405143738\n",
            "44/44 [==============================] - 22s 493ms/step - loss: 0.7913 - accuracy: 0.7327\n",
            "The score of Model 3 is\n",
            "0.7327089309692383\n",
            "44/44 [==============================] - 22s 503ms/step - loss: 0.8564 - accuracy: 0.7486\n",
            "The score of Model 4 is\n",
            "0.7485590577125549\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  score = model_[i].evaluate(X_validation,y_validation)\n",
        "  print(\"The score of Model \" + str(i) + \" is\")\n",
        "  print(score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_O8dHVZnELv"
      },
      "outputs": [],
      "source": [
        "print(type(model_[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eQO2c61Ruc0"
      },
      "source": [
        "# MODEL PREDICTION\n",
        "2 possible ways:\n",
        " - Highest of \"Sum of all the probabilities results\"\n",
        " - Most voted"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VsP36SY5RwiN"
      },
      "outputs": [],
      "source": [
        "#This function take in input the bagging model and the sample that need to make the prediction\n",
        "#Return the results of the prediction as softmax array\n",
        "def prediction(models,value,true_label):\n",
        "\n",
        "  #Create this to pass the value as input for our models\n",
        "  tensor = []\n",
        "  tensor.append(value)\n",
        "  tensor = np.array(tensor)\n",
        "  #Extrapolate the label from the softmax representation\n",
        "  label = np.argmax(true_label)\n",
        "  \n",
        "  #Doing the prediction on all 5 the models\n",
        "  y = []\n",
        "  for i in range(5):\n",
        "    y.append(models[i].predict(tensor, verbose = 0))\n",
        "  #Sum the results of all the models to obtain the final result\n",
        "  results = np.sum(y,axis=0)  \n",
        "  \n",
        "  max_index = np.unravel_index(np.argmax(results), results.shape)\n",
        "  \n",
        "  #Check if the value was at unanimity\n",
        "  unanimity = True\n",
        "  for i in range(len(models)):\n",
        "    if np.argmax(y[0]) != np.argmax(y[i]):\n",
        "      unanimity = False\n",
        "    \n",
        "  \n",
        "  #print(\"True Label: \" + str(label))\n",
        "  #print(\"Predicted label: \" + str(np.argmax(results)))\n",
        "  #Return True if the prediction was correct, and if the prediction was taken at the unanimity\n",
        "\n",
        "  return label == np.argmax(results), unanimity\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56pnNB2lo9S_",
        "outputId": "44a5c92e-1d48-44e6-f9de-a6c8731258ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f0da078b820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(True, True)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "prediction(model_,X_test[0],y_test[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JVh3p2K0cH6A",
        "outputId": "323418a1-fab1-4465-ed2d-3e23fd57b05f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of test set: 1336\n",
            "Positive at unanimity: 740\n",
            "Positive NOT at unanimity: 334\n",
            "Negative at unanimity: 48\n",
            "Negative NOT at unanimity: 214\n"
          ]
        }
      ],
      "source": [
        "#Use our function to evaluate boosting :\n",
        "positive_unanimity = 0\n",
        "positive = 0\n",
        "negative_unanimity = 0\n",
        "negative = 0\n",
        "for i in range(len(X_test)):\n",
        "  isCorrect, unanimity = prediction(model_,X_test[i],y_test[i])\n",
        "  if isCorrect:\n",
        "    if unanimity:\n",
        "      positive_unanimity = positive_unanimity + 1\n",
        "    else:\n",
        "      positive = positive + 1\n",
        "  else:\n",
        "    if unanimity:\n",
        "      negative_unanimity = negative_unanimity + 1\n",
        "    else:\n",
        "      negative = negative + 1\n",
        "\n",
        "print(\"Length of test set: \" + str(len(X_test)))\n",
        "print(\"Positive at unanimity: \" + str(positive_unanimity))\n",
        "print(\"Positive NOT at unanimity: \" + str(positive))\n",
        "print(\"Negative at unanimity: \" + str(negative_unanimity))\n",
        "print(\"Negative NOT at unanimity: \" + str(negative))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X91uj2mQ1uyj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16ab2121-7bb1-42fd-a798-264dc4246118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1074\n",
            "0.8038922155688623\n"
          ]
        }
      ],
      "source": [
        "total_positive = positive_unanimity + positive\n",
        "print(total_positive)\n",
        "positive_percentual = total_positive/len(X_test)\n",
        "print(positive_percentual)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"Model0.h5\")\n",
        "files.download(\"Model1.h5\")\n",
        "files.download(\"Model2.h5\")\n",
        "files.download(\"Model3.h5\")\n",
        "files.download(\"Model4.h5\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "994b7T7UwaVO",
        "outputId": "165d67cf-b822-4f42-ba8b-c16b864ad8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4cc49035-4045-4136-93e3-84920bd861e6\", \"Model0.h5\", 424128)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3b2c3faf-c4aa-45f6-9ae9-f9cfc9f07d0b\", \"Model1.h5\", 424128)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1e01fea3-de20-46fb-be51-e98037c6a9d7\", \"Model2.h5\", 424128)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_26571ca3-67f8-4aea-baaa-6804512b5795\", \"Model3.h5\", 424128)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_53eaf1b0-aa46-4703-bbef-480a660fffec\", \"Model4.h5\", 424264)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}